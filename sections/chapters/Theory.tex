\chapter{Theory} % What we understand

Theory speaks to which accepted theories the topic relies on.  One such assumption is that having more knowledge about a dataset will create machine learning models which perform better on the same data (Lecun et al., 2015).  The goal of extracting knowledge from a dataset for a machine learning model is often known as feature extraction.  Practitioners who perform better feature extraction create models that perform better.  This feature extraction consists of extracting information from the model.  Another aspect is the quality of those features, i.e. do the features predict data behavior (Guyon 2006).  The proposition of this paper is more knowledge, i.e. features can be extracted when considering text and image datasets together, rather than each separately.  These joint features provide new knowledge, therefore new features, which should result in knowledge gained and a potentially better performing final machine learning model.  

Another theory is that data types are not always mutually exclusive in their features, i.e. that correlations can exist between features in different data types (Srivastava 2014).  This theory has been shown to be true in videos by decomposing data into visual and audio data (Graves 2012).  Yet, this theory has less support between text and image datasets.  This research provides opportunity to affirm or prove false for datasets consisting of text and image data.  

Machine learning in the past decade has made random forest and boosting algorithms more popular.  These algorithms operate on the assumptions that adding tweaks to models via weaker models can improve model weak points (Schapire et al, 2002).  Random forests are an ensemble of decision trees, and boosting methods tend to use random forests to improve weak points in model performance.  The decision trees are a good choice for improving model weak points because they are quick to train and work well on small datasets.  It is assumed that the combined model training on a small dataset will converge quickly and operate similarly.  Moreover, ensemble and boosting methods operate on a general theory that tweaking models at weak points can significantly improve model performance.  Building on this theory, new methods that can improve model weakpoints can largely improve model performance.  

\input{sections/chapters/theory/PhilosophicalAssumptions}
