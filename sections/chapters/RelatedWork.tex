\section{Related Work}

Shen et. al explores attention mechanisms for machine learning.  The subject of attention mechanisms is not well known within machine learning.  It has recently attracted a large amount of attention, due it its performance and speed of computation.  Since attention mechanisms are more lightweight, they train faster.  The mechanism, as will be explained, still relies on nodes, and therefore has much of the flexibility of neural networks.  Shen et. al delve into a type of attention mechanisms, a recurrent attention and bi-way attention mechansism denoted as a Directional Self-Attention Network (DiSAN).  The paper shows that its DiSAN model outperforms complicated RNN models in prediction accuracy and time efficiency on existing benchmarks.  The paper is relevant because it presents another, quite new machine learning mechanism that has shown promise for applied machine learning.

The attention mechanism takes advantage of a hidden neural network layer.  The hidden layer works on the input sequence and predicts the importance of their weights.  This creates a mechanism where neural network inputs are scrutinized by a separate neural network.  The separate neural network determines the importance of the weights, and give credence to those weights, so that the model primarily uses the most important inputs.  The result is a categorical distribution for the input sequence, and the neural network nodes have memory of which input sequences are important or more relevant.  One disadvantage of these networks is that temporal order of input information is lost.  The paper's DiSAN model helps fix this by providing sequential memory for the attention networks.  The paper demonstrates that their attention mechanism models perform particularly well at alignment scores between two sources, i.e. does well at providing a similarity score between two sources or texts.

There are a few jewels in the Shen et. al paper, like how an additive function for attention often outperforms multiplicative attention, and is also more memory efficient.  Their models make us of cross-entropy as an optimization objective and include L2 regularization.  The minimization optimizer is Adadelta with mini-batch of size 64.  The initial learning rate is quite large, i.e. 0.5, which is decreased over epochs.  The weight matrices for networks use GloVe and are pre-trained with out of vocabularty words, which initially were randomely initialized from a uniform distribution.  The model uses a dropout of 0.25 and 0.2.  The dropout is also varied throughout the learning process.  The final model uses fewer parameters than either RNN or CNN networks by margins of 3\%.  The model is applied to the Standford Sentiment Treebank and performs better than the best existing model by 0.52\%.  The model is also applied to Sentences Involving Compositional Knowledge (SICK) and with a similar performance.  Of important note is the model's bi-directional ability to track different features in forward progressing layers than a backward focused layer, one picking up word families and the latter focusing on word carousel.


Ji Lee performs sequential short-text classification with ANNs. The author's point is that text classifications often occur by only considering a text, not necessarily its preceding or subsequent texts.  The paper proposes that using information preceding short texts may improve classification accuracy.  Their model initially generates vector representations for short-texts using either the RNN or CNN architectures.  The authors utilized early stopping after 10 epochs and performed hyperparameter training.  Their model serves as a benchmark for ANN performance to sequential short-text classification.

Wenpeng et al perform a comparative study between CNN and RNN for Natural Langauge Processing (NLP).  This is an interseting subject, since RNNs and CNNs differently model sentences.  RNNs capture units in sequence and CNNs are good at extracting positional invariant features.  Both CNNs and RNNS are also the primary types of DNNs.  The paper covers multiple NLP tasks with each type of network, specifically CNNs, Gated Recurrent Units (GRUs), and LSTMs.  It is wroth mentioning that the networks in the study did not obtain great performance on existing benchmarks, which may limit the value of the study's insights.  The NLP tasks are sentiment/relation classification, textual entailment, answer selection, question-relation matching, and part-of-speech tagging.  The authors found that both CNNs and RNNS provide complementary information on text classification tasks.  The authors also found that changing hidden layer sizes and batch sizes resulted in large performance fluctuations.  A related work in the study found that RNNs compute a weighted sum of n-grams while CNNs extract the most important n-grams and only consider their resulting activation.

I read a paper on sentence pair scoring by Petr et al.  The authors argue that many sentence pairing tasks like Answer Set Selection, Semantic Text Scoring, Next Utterance Ranking, and Recognizing Textual Entailment are all very similar.  They propose a unified framework that employs task-independent models for sentence pair scoring models.  The model can easily compare models against its baselne in an effort to create a better framework for evaluating machine learning models.  It could be worthy comparing any models I might create for sentence pair scoring within their model framework.

There were a few papers on deep learning with video data.  One interesting paper performed deep learning by using CNNs on mulitple frames.  This paper was by Hossein Mobahi.  The paper performs large-scale object recognition.  Videos are composed of multiple frames, which provides a number of frames which contain the same objects.  These similar frames can each be processed by a CNN to provide additional information and possibly better accuracy in object recognition.  The paper learns the objects, based on the frame-to-frame video motion by performing classification on each frame.  The authors see learning from multiple frames more related to evolution, as humans experience learning through the world, which is constantly moving and changing.  The paper made use of 72x72 sized images of 100 images, where each object was shot 100 times at angles that each differed by 5 degrees.

Wojciech Zaremba explores RNN regularization.  Dropout is the most successful technique for regularizing neural networks, but they do not work well with RNNS and Long Short-Term Memory units (LSTMs).  Dropout works by randomly dropping outputs on a certain percentage of nodes.  Dropout is used as a form of regularization to make networks more generic and stable on new inputs.  Being able to apply regularization to RNNs or LSTMs could make video deep learning much more performant.  Due to lack of regularization effectiveness in RNNs, RNNs tend to quickly overfit on large networks.  The author's trick is to apply the dropout operator only to the non-recurrent connections.  The final model uses minibatch of size 20 with 650 units per layer of the LSTM.

Graves from Google researched speech recognition with RNNs.  The paper presents a system to transcribe audio data to text.  The paper is novel because it performs transcription without a phonetic representation.  The model uses a bidirectional LSTM RNN architecture.  Current practice working from audio to text data comprises speaker normalization and vocal tract length normalization.  The normalized voice is then fed into a model.  The extra step of normalization of voices makes the model bad at dealing with outlier voices, such as the elderly.  By having the model direclty transcript the audio data, the model can overcome the difficulties of odd voice tracts.  There are papers by Graves that perform raw speech with RNNs and Restricted Boltzman Machines (RBMs), but the model is expensive and tends to be worse than conventional processing.  A previous work did speech recognition with this architecture (Eybern et al., 2009), but it used a shallow architecture and did not deliver compelling results. An advantage of this research is its use of bidirectional RNNs to capture the whole utterances and their context.  This is possibly useful in the core research proposed by the term paper as a method for analyzing the entire context of speech data.


