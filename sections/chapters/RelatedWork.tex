
\chapter{Related Work}

\subsubsection{Single Data Type Architectures}
One paper my Simonyan and Zisserman explore very deep CNNs on large-scale image recognition.  The authors found that the trick to having deep CNNs is to have small filters.  The authors had 3x3 CNN filters with 16-19 layers and their model are two of the best performing convnets publically available.  The authors performed no fine-tuning and fed their model fixed-size 224x224 RGB images.  The only preprocessing done is subtracting the mean RGB value computed on the training set from each pixel so that the pixel intensity values tend to fall in a semi-normal distribution around zero.  Surpisingly, the stride is kept at one, which is common in practice but requires more time for training.  Spatial pooling ocurs five times, which follow the conv layers, though sometimes the model has multiple conv layers before the data is pooled.  Max-pooling is performed on a small 2x2 window with a stride of 2, which likely saves a lot of model training time.  The authors used a ReLU on all layers and none of the layers contain Local Response Normalization (LRN), as such normalization only leads to great memory consumption and computation time.  The benefit to the many small filters is that the number of weights in the convnet is not greater than that of a shallow net with larger convnet layers.  The model utilized a mini-batch of 256 with momentum set to 0.9, which is a common pattern in many of these papers.  The model did include L2 normalization with the multiplier set to 5*10-4.  A dropout of 0.5 was used, which seems very large compared with most papers, which probably use 0.3.  The learning rate is initially set to 0.1 and decreased by a factor of 10 when the validation set accuracy stopped improving.  A common theme in the related work is a large learning rate to initialize weights, which then dramatically decrease for small performance increases.  The authors obtains fixed-size 224x224 convnet images by randomly cropping and rescaling training images.  The crops also underwent random horizontal flipping and random RGB color shifts.  The authors emphasized that using a large set of crops can lead to an improved accuracy.  The training time took 2-3 weeks and training with 1000 classes on 1.3M images, and tested on 100k images, and validated on 50k images.  

Ji Lee performs sequential short-text classification with ANNs. The author's point is that text classifications often occur by only considering a text, not necessarily its preceding or subsequent texts.  The paper proposes that using information preceding short texts may improve classification accuracy.  Their model initially generates vector representations for short-texts using either the RNN or CNN architectures.  The authors utilized early stopping after 10 epochs and performed hyperparameter training.  Their model serves as a benchmark for ANN performance to sequential short-text classification.

Shen et. al explores attention mechanisms for machine learning.  The subject of attention mechanisms is not well known within machine learning.  It has recently attracted a large amount of attention, due it its performance and speed of computation.  Since attention mechanisms are more lightweight, they train faster.  The mechanism, as will be explained, still relies on nodes, and therefore has much of the flexibility of neural networks.  Shen et. al delve into a type of attention mechanisms, a recurrent attention and bi-way attention mechansism denoted as a Directional Self-Attention Network (DiSAN).  The paper shows that its DiSAN model outperforms complicated RNN models in prediction accuracy and time efficiency on existing benchmarks.  The paper is relevant because it presents another, quite new machine learning mechanism that has shown promise for applied machine learning.

The attention mechanism takes advantage of a hidden neural network layer.  The hidden layer works on the input sequence and predicts the importance of their weights.  This creates a mechanism where neural network inputs are scrutinized by a separate neural network.  The separate neural network determines the importance of the weights, and give credence to those weights, so that the model primarily uses the most important inputs.  The result is a categorical distribution for the input sequence, and the neural network nodes have memory of which input sequences are important or more relevant.  One disadvantage of these networks is that temporal order of input information is lost.  The paper's DiSAN model helps fix this by providing sequential memory for the attention networks.  The paper demonstrates that their attention mechanism models perform particularly well at alignment scores between two sources, i.e. does well at providing a similarity score between two sources or texts.

There are a few jewels in the Shen et. al paper, like how an additive function for attention often outperforms multiplicative attention, and is also more memory efficient.  Their models make us of cross-entropy as an optimization objective and include L2 regularization.  The minimization optimizer is Adadelta with mini-batch of size 64.  The initial learning rate is quite large, i.e. 0.5, which is decreased over epochs.  The weight matrices for networks use GloVe and are pre-trained with out of vocabularty words, which initially were randomely initialized from a uniform distribution.  The model uses a dropout of 0.25 and 0.2.  The dropout is also varied throughout the learning process.  The final model uses fewer parameters than either RNN or CNN networks by margins of 3\%.  The model is applied to the Standford Sentiment Treebank and performs better than the best existing model by 0.52\%.  The model is also applied to Sentences Involving Compositional Knowledge (SICK) and with a similar performance.  Of important note is the model's bi-directional ability to track different features in forward progressing layers than a backward focused layer, one picking up word families and the latter focusing on word carousel.

\subsubsection{}
Ji et al. presents another method for recognizing human action with CNNs.  The topic is particularly interesting because it represents a model that translates between very different data types.   The approach is simple and uses a CNN to predict actions at the frame level.  Another CNN then takes inputs from contiguous frames via their location.  The end-result is a 3D CNN that combines each 2D frame and uses time as the third dimension.  The hope is that the CNN models will capture temporal information from the adjacent frames.  This seems like a very good method to represent the data with very little bias.  The model outperformed 2D CNNs, which seems sensible since the 3D model contains the 2D model plus more information.

Wenpeng et al perform a comparative study between CNN and RNN for Natural Langauge Processing (NLP).  This is an interseting subject, since RNNs and CNNs differently model sentences.  RNNs capture units in sequence and CNNs are good at extracting positional invariant features.  Both CNNs and RNNS are also the primary types of DNNs.  The paper covers multiple NLP tasks with each type of network, specifically CNNs, Gated Recurrent Units (GRUs), and LSTMs.  It is wroth mentioning that the networks in the study did not obtain great performance on existing benchmarks, which may limit the value of the study's insights.  The NLP tasks are sentiment/relation classification, textual entailment, answer selection, question-relation matching, and part-of-speech tagging.  The authors found that both CNNs and RNNS provide complementary information on text classification tasks.  The authors also found that changing hidden layer sizes and batch sizes resulted in large performance fluctuations.  A related work in the study found that RNNs compute a weighted sum of n-grams while CNNs extract the most important n-grams and only consider their resulting activation.

There were a few papers on deep learning with video data.  One interesting paper performed deep learning by using CNNs on mulitple frames.  This paper was by Hossein Mobahi.  The paper performs large-scale object recognition.  Videos are composed of multiple frames, which provides a number of frames which contain the same objects.  These similar frames can each be processed by a CNN to provide additional information and possibly better accuracy in object recognition.  The paper learns the objects, based on the frame-to-frame video motion by performing classification on each frame.  The authors see learning from multiple frames more related to evolution, as humans experience learning through the world, which is constantly moving and changing.  The paper made use of 72x72 sized images of 100 images, where each object was shot 100 times at angles that each differed by 5 degrees.


\subsubsection{RNNs}
Building on the topic of LSTMs within video representations, since many of these interact with both image and motion data.  Srivastava et al. created an unsupervised model for learning on video data with LSTMs with the ultimate goal of action recognition.  They cite one challenge as tracking multiple objects moving in a background.  The paper said that LSTM was useful at extracting and extrapolating motion beyond what the video observed, though that metric seems difficult to measure in terms of goodness or performance. The authors also took the approach of skip-gram models of trying to predict in-between frames to train their model. The final model predicted up to 13 frames in the future and took 20 hours to converge on only 300 hours of data. The resulting predictions and reconstructions were blurry.  The blurriness was fixed by adding more LSTM units to remember image data.  They faced the issue of LSTM and their gradients vanishing.  Despite this, they had 74.3\% accuracy on recognizing actions from video data, which seems respectible.  The authors found that the model often loss the ability to keep precise object features in future frames, though it could recreate long-term object motion. 

Alex Graves at Toronto persents a paper on predicting future handwriting using LSTMs.  The resulting system was able to generate highly realistic cursive handwriting in a wide variety of styles.  The topic is interesting because it is an example of using a different machine learning architecture to translate data from video to another format, i.e. predicted handwriting sequences.  The topic of translating data to a type of prediction is interesting for the term paper's topic.  Existing work has used LSTMs to generate future sequences in domains like music.  RNNs are nice because they are fuzzy in the sense that they do not use exact templates from the training data to make predictions but use internal representations to interpolate from the training data to a result.  This RNN reconstitution of training data is an interesting way to transform data from one type to another.  It might be interesting to use RNNs as a translator from one data format to another, then use that representation to train another machine learning model.  The paper builds on this principle and finds that a better data type translation occurs when the LSTMs are given longer memories.  The model uses skip connections to all input layers, which does not connect top RNN layers to bottom RNN layers, assuming that these connections are likely unrelated and unhelpful to the final model.  The paper also constrained its gradients to a smaller range to prevent large derivatives in the backpropagation.  The authors also found that retraining with iteratively increased regularization results in faster training than random weights with regularization.  This makes sense, since the initial weights are likely better than random weights.  Their network only took four epochs to converge.  It is good to know that LSTMs can converge so quickly.

Building on learning data type representations, Cho et al. builds upon phrase representations using RNN encode-decoders with the purpose of language modeling.  The authors use two RNNs as an encoder-decoder pair.  There is definitely an emerging trend in the related work where RNNs are used to create internal data representations for encoding data to another data type.  The model translates from English to French and learns the translation probabilty of an English phrase corresponding to a French phrase.  The model can conversly be used to score a given pair of input and output sequences. The authors also acknowledged that simply training statistical models do not necessarily lead to the optimal performance.

Wojciech Zaremba explores RNN regularization.  Dropout is the most successful technique for regularizing neural networks, but they do not work well with RNNS and Long Short-Term Memory units (LSTMs).  Dropout works by randomly dropping outputs on a certain percentage of nodes.  Dropout is used as a form of regularization to make networks more generic and stable on new inputs.  Being able to apply regularization to RNNs or LSTMs could make video deep learning much more performant.  Due to lack of regularization effectiveness in RNNs, RNNs tend to quickly overfit on large networks.  The author's trick is to apply the dropout operator only to the non-recurrent connections.  The final model uses minibatch of size 20 with 650 units per layer of the LSTM.

\subsubsection{Architectures Will Multiple Data Types}
Karen et al. proposes a unique use of combining CNNs for action recognition in videos.  The goal is to capture complementary information from still frames and their motion. The recognition of human actions in video is well researched.  This paper builds upon existing works by creating a new architecture for analyzing human actions by combining an image-based model and a movement-based model, e.g. one model tracks the still images and the other tracks the gradient of movement in those images.  In this way, the paper presents a technique to combine two different data types into a single CNN model.  The overall idea is the motion and still-frame data types are very different. Also, actions often contain motion.  The action motion can aid to the identification of the action.  The CNNs are separately trained and later combined via their softmax scores.  The paper does some interesting calculations for the motion in order to account for a moving camera by subtracting movement that exists across the entire frame.  The CNNs do max-pooling with a 3x3 window and a stride of 2, which seemed practically too large.  The images were 224x224 and randomely cropped, horizontally flipped, and underwent RGB jittering.  The authors found that such fine-tuning only gave marginal improvements over the training set.  The paper also saw that large dropout over-regularises learning and leads to a worse overall accuracy.

Translating images to videos is a popular topic.  It is particularly interesting because it performs translation from one data type to another.  Donahue et al presents a model transcribing video to text descriptions.  The paper proposes an architecture known as Long-term Recurrent Convolutional Networks (LRCNs).  The goal of the architecture is to levarage CNN recognition strengths and to have an RNN remember time-varying inputs and outputs. One of the largest difficulties with the model is deciding how much time-varying information to remember, which were not deterministic in this model due to their issue of the vanishing gradient in their RNN model.  The authors see their model as an improvement on video activity datasets with complex time dynamics and improve existing benchamrks by 4\%.  The LRCN predicts the video class at each time step and average the predictions for a final classification.  The model extracts from 16 frame clips and uses a stride of 8 frames from each video.  The model is trained on TACoS, a dataset for video/sentence pairs.

Graves from Google researched speech recognition with RNNs.  The paper presents a system to transcribe audio data to text.  The paper is novel because it performs transcription without a phonetic representation.  The model uses a bidirectional LSTM RNN architecture.  Current practice working from audio to text data comprises speaker normalization and vocal tract length normalization.  The normalized voice is then fed into a model.  The extra step of normalization of voices makes the model bad at dealing with outlier voices, such as the elderly.  By having the model direclty transcript the audio data, the model can overcome the difficulties of odd voice tracts.  There are papers by Graves that perform raw speech with RNNs and Restricted Boltzman Machines (RBMs), but the model is expensive and tends to be worse than conventional processing.  A previous work did speech recognition with this architecture (Eybern et al., 2009), but it used a shallow architecture and did not deliver compelling results. An advantage of this research is its use of bidirectional RNNs to capture the whole utterances and their context.  This is possibly useful in the core research proposed by the term paper as a method for analyzing the entire context of speech data.










