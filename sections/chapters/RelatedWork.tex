
Shen et. al explores attention mechanisms for machine learning.  The subject of attention mechanisms is not well known within machine learning.  It has recently attracted a large amount of attention, due it its performance and speed of computation.  Since attention mechanisms are more lightweight, they train faster.  The mechanism, as will be explained, still relies on nodes, and therefore has much of the flexibility of neural networks.  Shen et. al delve into a type of attention mechanisms, a recurrent attention and bi-way attention mechansism denoted as a Directional Self-Attention Network (DiSAN).  The paper shows that its DiSAN model outperforms complicated RNN models in prediction accuracy and time efficiency on existing benchmarks.  The paper is relevant because it presents another, quite new machine learning mechanism that has shown promise for applied machine learning.

The attention mechanism takes advantage of a hidden neural network layer.  The hidden layer works on the input sequence and predicts the importance of their weights.  This creates a mechanism where neural network inputs are scrutinized by a separate neural network.  The separate neural network determines the importance of the weights, and give credence to those weights, so that the model primarily uses the most important inputs.  The result is a categorical distribution for the input sequence, and the neural network nodes have memory of which input sequences are important or more relevant.  One disadvantage of these networks is that temporal order of input information is lost.  The paper's DiSAN model helps fix this by providing sequential memory for the attention networks.  The paper demonstrates that their attention mechanism models perform particularly well at alignment scores between two sources, i.e. does well at providing a similarity score between two sources or texts.

There are a few jewels in the Shen et. al paper, like how an additive function for attention often outperforms multiplicative attention, and is also more memory efficient.  Their models make us of cross-entropy as an optimization objective and include L2 regularization.  The minimization optimizer is Adadelta with mini-batch of size 64.  The initial learning rate is quite large, i.e. 0.5, which is decreased over epochs.  The weight matrices for networks use GloVe and are pre-trained with out of vocabularty words, which initially were randomely initialized from a uniform distribution.  The model uses a dropout of 0.25 and 0.2.  The dropout is also varied throughout the learning process.  The final model uses fewer parameters than either RNN or CNN networks by margins of 3\%.  The model is applied to the Standford Sentiment Treebank and performs better than the best existing model by 0.52\%.  The model is also applied to Sentences Involving Compositional Knowledge (SICK) and with a similar performance.  Of important note is the model's bi-directional ability to track different features in forward progressing layers than a backward focused layer, one picking up word families and the latter focusing on word carousel.


