\section{Related Work}

Shen et. al explores attention mechanisms for machine learning.  The subject of attention mechanisms is not well known within machine learning.  It has recently attracted a large amount of attention, due it its performance and speed of computation.  Since attention mechanisms are more lightweight, they train faster.  The mechanism, as will be explained, still relies on nodes, and therefore has much of the flexibility of neural networks.  Shen et. al delve into a type of attention mechanisms, a recurrent attention and bi-way attention mechansism denoted as a Directional Self-Attention Network (DiSAN).  The paper shows that its DiSAN model outperforms complicated RNN models in prediction accuracy and time efficiency on existing benchmarks.  The paper is relevant because it presents another, quite new machine learning mechanism that has shown promise for applied machine learning.

The attention mechanism takes advantage of a hidden neural network layer.  The hidden layer works on the input sequence and predicts the importance of their weights.  This creates a mechanism where neural network inputs are scrutinized by a separate neural network.  The separate neural network determines the importance of the weights, and give credence to those weights, so that the model primarily uses the most important inputs.  The result is a categorical distribution for the input sequence, and the neural network nodes have memory of which input sequences are important or more relevant.  One disadvantage of these networks is that temporal order of input information is lost.  The paper's DiSAN model helps fix this by providing sequential memory for the attention networks.  The paper demonstrates that their attention mechanism models perform particularly well at alignment scores between two sources, i.e. does well at providing a similarity score between two sources or texts.

There are a few jewels in the Shen et. al paper, like how an additive function for attention often outperforms multiplicative attention, and is also more memory efficient.  Their models make us of cross-entropy as an optimization objective and include L2 regularization.  The minimization optimizer is Adadelta with mini-batch of size 64.  The initial learning rate is quite large, i.e. 0.5, which is decreased over epochs.  The weight matrices for networks use GloVe and are pre-trained with out of vocabularty words, which initially were randomely initialized from a uniform distribution.  The model uses a dropout of 0.25 and 0.2.  The dropout is also varied throughout the learning process.  The final model uses fewer parameters than either RNN or CNN networks by margins of 3\%.  The model is applied to the Standford Sentiment Treebank and performs better than the best existing model by 0.52\%.  The model is also applied to Sentences Involving Compositional Knowledge (SICK) and with a similar performance.  Of important note is the model's bi-directional ability to track different features in forward progressing layers than a backward focused layer, one picking up word families and the latter focusing on word carousel.


Ji Lee performs sequential short-text classification with ANNs. The author's point is that text classifications often occur by only considering a text, not necessarily its preceding or subsequent texts.  The paper proposes that using information preceding short texts may improve classification accuracy.  Their model initially generates vector representations for short-texts using either the RNN or CNN architectures.  The authors utilized early stopping after 10 epochs and performed hyperparameter training.  Their model serves as a benchmark for ANN performance to sequential short-text classification.

Wenpeng et al perform a comparative study between CNN and RNN for Natural Langauge Processing (NLP).  This is an interseting subject, since RNNs and CNNs differently model sentences.  RNNs capture units in sequence and CNNs are good at extracting positional invariant features.  Both CNNs and RNNS are also the primary types of DNNs.  The paper covers multiple NLP tasks with each type of network, specifically CNNs, Gated Recurrent Units (GRUs), and LSTMs.  It is wroth mentioning that the networks in the study did not obtain great performance on existing benchmarks, which may limit the value of the study's insights.  The NLP tasks are sentiment/relation classification, textual entailment, answer selection, question-relation matching, and part-of-speech tagging.  The authors found that both CNNs and RNNS provide complementary information on text classification tasks.  The authors also found that changing hidden layer sizes and batch sizes resulted in large performance fluctuations.  A related work in the study found that RNNs compute a weighted sum of n-grams while CNNs extract the most important n-grams and only consider their resulting activation.

I read a paper on sentence pair scoring by Petr et al.  The authors argue that many sentence pairing tasks like Answer Set Selection, Semantic Text Scoring, Next Utterance Ranking, and Recognizing Textual Entailment are all very similar.  They propose a unified framework that employs task-independent models for sentence pair scoring models.  The model can easily compare models against its baselne in an effort to create a better framework for evaluating machine learning models.  It could be worthy comparing any models I might create for sentence pair scoring within their model framework.

There were a few papers on deep learning with video data.  One interesting paper performed deep learning by using CNNs on mulitple frames.  This paper was by Hossein Mobahi.  The paper performs large-scale object recognition.  Videos are composed of multiple frames, which provides a number of frames which contain the same objects.  These similar frames can each be processed by a CNN to provide additional information and possibly better accuracy in object recognition.  The paper learns the objects, based on the frame-to-frame video motion by performing classification on each frame.  The authors see learning from multiple frames more related to evolution, as humans experience learning through the world, which is constantly moving and changing.  The paper made use of 72x72 sized images of 100 images, where each object was shot 100 times at angles that each differed by 5 degrees.

Wojciech Zaremba explores RNN regularization.  Dropout is the most successful technique for regularizing neural networks, but they do not work well with RNNS and Long Short-Term Memory units (LSTMs).  Dropout works by randomly dropping outputs on a certain percentage of nodes.  Dropout is used as a form of regularization to make networks more generic and stable on new inputs.  Being able to apply regularization to RNNs or LSTMs could make video deep learning much more performant.  Due to lack of regularization effectiveness in RNNs, RNNs tend to quickly overfit on large networks.  The author's trick is to apply the dropout operator only to the non-recurrent connections.  The final model uses minibatch of size 20 with 650 units per layer of the LSTM.

Graves from Google researched speech recognition with RNNs.  The paper presents a system to transcribe audio data to text.  The paper is novel because it performs transcription without a phonetic representation.  The model uses a bidirectional LSTM RNN architecture.  Current practice working from audio to text data comprises speaker normalization and vocal tract length normalization.  The normalized voice is then fed into a model.  The extra step of normalization of voices makes the model bad at dealing with outlier voices, such as the elderly.  By having the model direclty transcript the audio data, the model can overcome the difficulties of odd voice tracts.  There are papers by Graves that perform raw speech with RNNs and Restricted Boltzman Machines (RBMs), but the model is expensive and tends to be worse than conventional processing.  A previous work did speech recognition with this architecture (Eybern et al., 2009), but it used a shallow architecture and did not deliver compelling results. An advantage of this research is its use of bidirectional RNNs to capture the whole utterances and their context.  This is possibly useful in the core research proposed by the term paper as a method for analyzing the entire context of speech data.

Karen et al. proposes a unique use of combining CNNs for action recognition in videos.  The goal is to capture complementary information from still frames and their motion. The recognition of human actions in video is well researched.  This paper builds upon existing works by creating a new architecture for analyzing human actions by combining an image-based model and a movement-based model, e.g. one model tracks the still images and the other tracks the gradient of movement in those images.  In this way, the paper presents a technique to combine two different data types into a single CNN model.  The overall idea is the motion and still-frame data types are very different. Also, actions often contain motion.  The action motion can aid to the identification of the action.  The CNNs are separately trained and later combined via their softmax scores.  The paper does some interesting calculations for the motion in order to account for a moving camera by subtracting movement that exists across the entire frame.  The CNNs do max-pooling with a 3x3 window and a stride of 2, which seemed practically too large.  The images were 224x224 and randomely cropped, horizontally flipped, and underwent RGB jittering.  The authors found that such fine-tuning only gave marginal improvements over the training set.  The paper also saw that large dropout over-regularises learning and leads to a worse overall accuracy.

Translating images to videos is a popular topic.  It is particularly interesting because it performs translation from one data type to another.  Donahue et al presents a model transcribing video to text descriptions.  The paper proposes an architecture known as Long-term Recurrent Convolutional Networks (LRCNs).  The goal of the architecture is to levarage CNN recognition strengths and to have an RNN remember time-varying inputs and outputs. One of the largest difficulties with the model is deciding how much time-varying information to remember, which were not deterministic in this model due to their issue of the vanishing gradient in their RNN model.  The authors see their model as an improvement on video activity datasets with complex time dynamics and improve existing benchamrks by 4\%.  The LRCN predicts the video class at each time step and average the predictions for a final classification.  The model extracts from 16 frame clips and uses a stride of 8 frames from each video.  The model is trained on TACoS, a dataset for video/sentence pairs.

Ji et al. presents another method for recognizing human action with CNNs.  The topic is particularly interesting because it represents a model that translates between very different data types.   The approach is simple and uses a CNN to predict actions at the frame level.  Another CNN then takes inputs from contiguous frames via their location.  The end-result is a 3D CNN that combines each 2D frame and uses time as the third dimension.  The hope is that the CNN models will capture temporal information from the adjacent frames.  This seems like a very good method to represent the data with very little bias.  The model outperformed 2D CNNs, which seems sensible since the 3D model contains the 2D model plus more information.

Alex Graves at Toronto persents a paper on predicting future handwriting using LSTMs.  The resulting system was able to generate highly realistic cursive handwriting in a wide variety of styles.  The topic is interesting because it is an example of using a different machine learning architecture to translate data from video to another format, i.e. predicted handwriting sequences.  The topic of translating data to a type of prediction is interesting for the term paper's topic.  Existing work has used LSTMs to generate future sequences in domains like music.  RNNs are nice because they are fuzzy in the sense that they do not use exact templates from the training data to make predictions but use internal representations to interpolate from the training data to a result.  This RNN reconstitution of training data is an interesting way to transform data from one type to another.  It might be interesting to use RNNs as a translator from one data format to another, then use that representation to train another machine learning model.  The paper builds on this principle and finds that a better data type translation occurs when the LSTMs are given longer memories.  The model uses skip connections to all input layers, which does not connect top RNN layers to bottom RNN layers, assuming that these connections are likely unrelated and unhelpful to the final model.  The paper also constrained its gradients to a smaller range to prevent large derivatives in the backpropagation.  The authors also found that retraining with iteratively increased regularization results in faster training than random weights with regularization.  This makes sense, since the initial weights are likely better than random weights.  Their network only took four epochs to converge.  It is good to know that LSTMs can converge so quickly.

Building on learning data type representations, Cho et al. builds upon phrase representations using RNN encode-decoders with the purpose of language modeling.  The authors use two RNNs as an encoder-decoder pair.  There is definitely an emerging trend in the related work where RNNs are used to create internal data representations for encoding data to another data type.  The model translates from English to French and learns the translation probabilty of an English phrase corresponding to a French phrase.  The model can conversly be used to score a given pair of input and output sequences. The authors also acknowledged that simply training statistical models do not necessarily lead to the optimal performance. 
