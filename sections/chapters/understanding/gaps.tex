\subsection{What is the gap between what is known and what needs to be known?}

What is known is how to individually analyze data types with a great degree of accuracy.  There are also known techniques for translating data types to other types, such as transcribing actions in videos to text, or object recognition in images.  Data scientists have had success finding correlations between text and image data.  There are a few existing approaches.  One such approach is to extract features from each data type.  These features can then be fed into a NN that can find patterns within each data type.  Yet, extracting these features is complicated.  Features can be manually created or found by CNN or NN.  Yet, running data through a CNN or NN is time intensive and doesn't always result in meaningful featurs.  Feeding unmeaningful features into a third model might work, but it might not.  Moreover, extracting the initial features requires training a model on outputs. Each trained model is going to be biased to sense data type specific nuances, since that is how each model was trained.  The network might have poor weights for sensing interactions between data types.  All these situations are likely to lead to a poor result when combingin the outputs of different machine learning models.

While combining models sounds simple, there are many ways to combine the models.  The models can be combined at the final layer, the n-1 layer, the n-1 layer etc, or even have their inputs run through the same model until the model converges.  It is a difficult scenario and deserves research. This area of research is newer and opportune for exploration by a new researcher.  
