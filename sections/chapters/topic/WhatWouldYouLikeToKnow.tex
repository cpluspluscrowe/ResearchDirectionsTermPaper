\subsubsection{What would you like to know?}

I would like to know if LSTMs are capable of creating internal representations of each data type, so that LSTM models from each model can be combined.  I am curious if attention models can be used with single hidden layers to specify which data relationships between text and image data are most important/significant.  I am also curious how combining both text and image data in a CNN affects the data learned in each layer.  It would also be interesting to train a model on one data type, and then fix the top layers are retrain lower layers on the other data type to see if this gives performance benefits.  Such a fixed top/trained bottom data set might gain accuracy from learning some of the information from the other data set.  It would also be interesting to iteratively build a combined model based on two individual models, such as building a third combined model using weights from each independent image-CNN and text-NN model.  
