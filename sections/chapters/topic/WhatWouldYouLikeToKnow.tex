\subsection{What would you like to know?}

It would be worthwhile to apply the methodology set forth in future sections to existing multi-data type benchmarks like Flickr 8k and 30k.  Such an analysis would shed light on the feasibility of the methodology.  Part of the methodology uses boosting.  It would be interesting to see the use of boosting with Deep Neural Networks (DNNs).  Boosting is not commonly applied to DNNs since there really is not a concept of a weak model with NN.  NN, contrasted with weak models like decision trees, can learn massive featuresets with controlled overfitting.  This makes single DNNs appropriate models for boosting.  It will be interesting to see if the proposed concept of a NN weak model for boosting shows performance gains on existing benchmarks.  It will also be interesting if the proposed combined architecture yields any weak models, which might imply that the architecture identified either weak single-type or joint data type features.



