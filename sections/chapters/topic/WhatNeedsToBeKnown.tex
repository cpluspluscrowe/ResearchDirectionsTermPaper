\subsection{What needs to be known?}

Research in multi-data types have assumed that LSTMs and autoencoders work well at processing combined inputs.  Yet, this has not been shown to be true.  Rather, research by Ngiam et al. found that RMBs tended to not learn combined representations because the single data type signals were strong enough to control the network weights.  Even when LSTMs have been used with multi-data type datasets, they have only been used to translate from one data type to another.  Moreover, LSTM performance on multi-data type datasets was largely a factor of increased memory (Ji Lee, YEAR).

The actual performance of LSTMs, RBMs, or standard models like NN or CNNs with multi-data types have not been established.  The existing assumptions of researchers that RBMs work well on multi-data type datasets has been shown to have large limitations.  This area of study requires that benchmarks to be created for model performance with multi-data types.

Research with RBMs and multi-data types have shown these architectures exhibit some success at processing an input of multiple data types.  Yet, the research has not shown that RBMs capture joint dataset features.  Other techniques or architectures need to be employed in order to capture these joint dataset features.

A large research gap is detecting that joint data type features exist.  Research has no methods for detecting if correlated features exist between two data types.  The knowledge that correlated features exist give incentive to data users to exploit these correlations to create better machine learning models.   This paper sees the ability to detect correlated features within multi-data type datasets as a significant knowledge gap that could aid analysis future researchers and help train many datasets.
